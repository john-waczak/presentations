<!doctype html>
<html lang="en" prefix="og: http://ogp.me/ns#">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Automatic Differentiation, Differentiable Programming, and Scientific Machine Learning</title>

    <!-- Add MathJax Support -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>


    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,700,700i%7CMaitree:200,300,400,600,700&amp;subset=latin-ext" rel="stylesheet">
    <!-- CSS WebSlides -->
    <link rel="stylesheet" type='text/css' media='all' href="../static/css/webslides.css">
    <!-- Optional - CSS SVG Icons (Font Awesome) -->
    <link rel="stylesheet" type='text/css' media='all' href="../static/css/svg-icons.css">
  </head>
  <body>
    <main role="main">
        <article id="webslides" class="vertical">

            <!-- slide 1 -->
            <section>
                <span class="background" style="background-image:url('media/matrix.jpg')"></span>
                <!--.wrap = container (width: 90%) -->
                <div class="wrap aligncenter">
                    <h1 style="color:white"><strong>Automatic Differentiation, Differentiable Programming, and Scientific Machine Learning</strong></h1>
                    <h2 style="color:white">John Waczak </h2>
                    <h3 style="color:white">September 17, 2021</h3>
                    <h3 style="color:white">GSP</h3>
                </div>
            </section>

            <!-- slide 2 -->
            <section>
                <div class="bg-white shadow">
                    <h1><strong>Outline</strong></h1>
                    <ul class="flexblock reasons">
                        <li>
                            <h1>Automatic Differentiation (Autograd)</h1>
                            <h2>Analytic differentiation, numerical differentiation, dual numbers, forward mode AD, computational graphs, reverse mode AD, static graph vs backpropagation</h2>
                        </li>
                        <li>
                            <h1>Differentiable Programming (\(\partial P\))</h1>
                            <h2>Language wide AD as a programming paradigm, my pitch for Julia, demos: crappy sine function implementation</h2>
                        </li>
                        <li>
                            <h1>Scientific Machine Learning (SciML)</h1>
                            <h2>What is a NN really?, NN from scratch, physics informed NNs, Neural ODEs and autograd through DiffEqs, UDEs, SINDy and sparse regression, Fun example: Hamiltonian NNs</h2>
                        </li>
                    </ul>
                </div>
            </section>

            <!-- Analytic Differentiation -->
            <section class="bg-apple">
                <div class="aligncenter">
                    <h1><strong>Differentiation Techniques</strong></h1>
                </div>
            </section>
            <section class="slide-top">
                <div class="alignleft">
                    <h3>Analytic differentiation is nice but often nasty </h3>
                    <h3>
                        $$f'(x) = \lim_{\epsilon\to 0} \frac{f(x+\epsilon)-f(x)}{\epsilon}$$
                    </h3>
                    <h3>To make things easier, we can derive a set of algebraic rules for derivatives</h3>
                </div>
                <div class="wrap">
                    <div class="grid">
                        <div class="column">
                            <p>
                                $$\frac{d}{dx} c f(x) = c\cdot f'(x) $$
                                $$\frac{d}{dx}\left(f(x)+g(x)\right) = f'(x) + g'(x) $$
                                $$\frac{d}{dx}f(x)g(x) = f'(x)g(x) + f(x)g'(x) $$
                                $$\frac{d}{dx}f(g(x)) = f'(g(x))\cdot g'(x) $$
                            </p>
                        </div>
                        <div class="column">
                            <p>
                                $$\frac{d}{dx} x^a = ax^{a-1}$$
                                $$\frac{d}{dx}e^x = e^x$$
                                $$\frac{d}{dx}\sin(x) = \cos(x)$$
                                $$\frac{d}{dx}\cos(x) = -\sin(x)$$
                            </p>
                        </div>
                    </div>
                </div>
            </section>
            <section class="aligncenter">
                <h1>What do you do if you're given data and not functions?</h1>
            </section>

            <!-- Numerical Differentiation -->
            <section class="aligncenter">
                <h1><strong>Numerical Differentiation</strong></h1>
            </section>

            <section class="slide-top">
                <div class="alignleft">
                    <h3>Forward Difference: $$\Delta_F f(x) = f(x+\Delta x)-f(x) $$ </h3>
                    <h3>Backward Difference: $$\Delta_B f(x) = f(x)-f(x-\Delta x) $$ </h3>
                    <h3>Central Difference: $$\Delta_C f(x) = f(x+\Delta x/2)-f(x-\Delta x/2) $$ </h3>
                    <h3>$$ \vdots $$ </h3>
                    <h3> Adaptive step size, Runge-Kutta, etc...</h3>
                </div>
            </section>

            <section>
                <div class="aligncenter">
                    <h2><strong>These methods are great but their accuracy is limited by </strong></h2>
                    <br>
                    <h3> The resolution of our data (i.e. what is \(\Delta t\)) </h3>
                    <br>
                    <h3> and </h3>
                    <br>
                    <h3> The order of our method, i.e. \(\mathcal{O}(1), \mathcal{O}(2), \mathcal{O}(3), \mathcal{O}(4),\) etc...</h3>
                    <br>
                    <br>
                    <br>
                    <br>
                    <h3>Can we do better?</h3>
                </div>
            </section>


            <!-- Dual Numbers -->
            <section class="bg-apple">
                <div class="aligncenter">
                    <h1>The best of both worlds:</h1>
                    <h1><strong>Dual Numbers and Automatic Differentiation</strong></h1>
                </div>
            </section>


            <section class="aligncenter">
                <h1>A Motivating Example: Differentiation Over \(\mathbb{C}\) Valued Functions</h1>
            </section>

            <section class="aligncenter">
                <h1>A Motivating Example: Differentiation Over \(\mathbb{C}\) Valued Functions</h1>
            </section>

            <section class="slide-top">
                <div class="alignleft">
                    <h4>Consider a \(\mathbb{C}\)-valued function </h4>
                    <h4>$$f(x+i\delta)$$</h4>
                    <h4>If it's analytic (a strong requirement), then we can expand:</h4>
                    <h4>$$f(x+i\delta) = f(x) + if'(x)\delta + \mathcal{O}(\delta^2)$$</h4>
                    <h4>so that...</h4>
                    <h4>$$if'(x) = \frac{f(x+i\delta) - f(x)}{\delta} + \mathcal{O}(\delta)$$</h4>
                    <h4> which leads to the astonishing result </h4>
                    <h4>$$f'(x) \equiv \frac{\text{Im}(f(x+i\delta))}{\delta}+\mathcal{O}(\delta)$$ </h4>
                    <h3><strong>The imaginary part keeps track of the derivative!</strong></h3>
                </div>
            </section>

            <section class="aligncenter">
                <h1><strong>Dual Numbers</strong></h1>
            </section>

            <section>
                <div class="aligncenter">
                    <h3>Complex numbers are a subset of (pseudo) number fields called <i>Hypercomplex Numbers</i></h3>
                    <h3>These are numbers of the form \(a+\epsilon b\) where \(\epsilon\) satisfies one of 3 possibilities:</h3>
                    <br>
                </div>
                <div class="grid">
                    <div class="column">
                        <div class="bg-white shadow">
                            <h3><u>Complex Numbers:</u></h3>
                            <h3>\(\epsilon^2=-1\)</h3>
                        </div>
                    </div>
                    <div class="column">
                        <div class="bg-white shadow">
                            <h3><strong><u>Dual Numbers:</u></strong></h3>
                            <h3>\(\epsilon^2=0\) </h3>
                            <h4>If this bothers you, check out <a href="https://en.wikipedia.org/wiki/Smooth_infinitesimal_analysis">smooth infinitesmial analysis</a> </h4>
                        </div>
                    </div>
                    <div class="column">
                        <div class="bg-white shadow">
                            <h3><u>Split Complex Numbers:</u></h3>
                            <h3>\(\epsilon^2=1\)</h3>
                            <h3>also known as <a href="https://en.wikipedia.org/wiki/Split-complex_number#:~:text=In%20algebra%2C%20a%20split%20complex,%E2%88%97%20%3D%20x%20%E2%88%92%20y%20j.&text=This%20composition%20of%20N%20over,%C3%97%2C%20*)%20a%20composition%20algebra."> hyperbolic numbers</a></h3>
                        </div>
                    </div>
                </div>
            </section>

            <section class="slide-top">
                <div class="alignleft">
                    <h3>How does this work with functions of dual numbers? <i>Again, use a Taylor Expansion</i></h3>
                    <h3>$$f(x+\epsilon) = f(x) + f'(x)\epsilon + \cancel{\frac{f''(x)}{2}\epsilon^2 + \cdots}  $$</h3>
                    <h3> since \(\epsilon^2=0\).</h3>
                    <h3> This leads to the astonishingly usefull result. For the unital algebra of dual numbers: </h3>
                    <h3> $$\boxed{f(x+\epsilon) = f(x) + f'(x)\epsilon} $$ </h3>
                    <h3> If we can create a numeric type for dual numbers, <strong>we can automatically determine the derivative when we evaluate a function.</strong></h3>
                </div>
            </section>

            <section class="slide-top">
                <div class="aligncenter">
                    <h3>We can now use the best of both worlds of analytic differentiation and numerical methods</h3>
                    <h3> Let's define how the <i>algebra</i> of dual numbers behaves. let \(f(x)\) and \(g(x)\) be two functions. Then,</h3>
                    <br>
                    <br>
                    <div class="aligncenter bg-white shadow">
                        <h3><strong>Sum rule:</strong> \( (f+g)(x+\epsilon) = [f(x)+g(x)] + \epsilon[f'(x)+g'(x)] \) </h3>
                        <h3><strong>Product rule:</strong> \( (f\cdot g)(x+\epsilon) = [f(x)\cdot g(x)] + \epsilon[f(x)\cdot g'(x) + g(x)\cdot f'(x)]\)</h3>
                    </div>
                    <br>
                    <br>
                    <h3> And that's all we need! </h3>
                    <h3> This is easy enough to <u>turn into code</u></h3>
                </div>
            </section>

            <section class="slide-top">
                <h1><strong> Dual Numbers in code </strong></h1>
                <div class="wrap">
                    <div class="grid sm">
                        <div class="column">
                            <h4>Define a new data structure for dual numbers</h4>
                        </div>
                        <div class="column bg-white shadow">
                            <img class="alignright" src="media/dualNumbers-1.png" alt="dual numbers 1">
                        </div>
                    </div>
                </div>
                <div class="wrap">
                    <div class="grid sm">
                        <div class="column">
                            <h4>Define dispatches for standard arithmetic operations</h4>
                        </div>
                        <div class="column bg-white shadow">
                            <img class="alignright" src="media/dualNumbers-2.png" alt="dual numbers 2">
                        </div>
                    </div>
                </div>
                <div class="wrap">
                    <div class="grid sm">
                        <div class="column">
                            <h4>Define primitives for functions with known analytic derivatives </h4>
                        </div>
                        <div class="column bg-white shadow">
                            <img class="alignright" src="media/dualNumbers-3.png" alt="dual numbers 3">
                        </div>
                    </div>
            </section>

            <section class="slide-top">
                <div class="alignleft">
                    <h3>We can easily extend this to higher dimensions. Let \( f:\mathbb{R}^n\to\mathbb{R} \) and define \(\vec{\epsilon} := (\epsilon_1,...,\epsilon_n) \), then</h3>
                    <h3>$$ f(\vec{x}+\vec{\epsilon}) := f(\vec{x}) +\nabla f(\vec{x})\cdot \vec{\epsilon} $$ </h3>
                    <h3> which behaves as desired with properties: </h3>
                    <h3>
                        \begin{align}
                        (f+g)(\vec{x}+\vec{\epsilon}) &= [f(\vec{x})+g(\vec{x})] + [\nabla f(\vec{x}) + \nabla g(\vec{x})]\cdot \vec{\epsilon} \\ \\
                        (f\cdot g)(\vec{x}+\vec{\epsilon}) &= f(\vec{x})g(\vec{x}) + [f(\vec{x})\nabla g(\vec{x}) + g(\vec{x})\nabla f(\vec{x})]\cdot \vec{\epsilon}
                        \end{align}
                    </h3>
                    <h3>with the stipulation that \( \epsilon_i\epsilon_j = 0 \). For full generality, we can find the jacobian of a function \( f:\mathbb{R}^n\to\mathbb{R}^m \) by applying the above to each component function \(f_i(\vec{x})\) with \(i\in \{1,...,m\} \)</h3>
                    <br>
                    <br> 
                    <h3> This method of passing derivatives through nested multivariable functions is called <strong>Forward Mode Automatic Differentiation </strong></h3>
                </div>
            </section>

            <!-- Reverse Mode AD -->
            <section class="bg-apple">
                <div class="aligncenter">
                    <h1>Computational Graphs and</h1>
                    <h1><strong>Reverse Mode Automatic Differentiation</strong></h1>
                </div>
            </section>

            <section>
                <div class="aligncenter">
                    <h1>Forward Mode AD works well, but quickly becomes computationally expensive.</h1>
                    <br>
                    <br>
                    <h1>What do you do if your model has <i>thousands</i> of functions with <i>hundreds of thousands</i> of tunable parameters?</h1>
                </div>
            </section>

            <section class="slide-top">
                <div class="alignleft">
                    <h2><strong>The Chain Rule Revisited</strong></h2>
                    <h3>Suppose we have a function \( f(x(t), y(t)) \), then </h3>
                    <h3>$$\frac{df}{dt} = \frac{df}{dx}\frac{dx}{dt} + \frac{df}{dy}\frac{dy}{dt} $$</h3>
                    <h3>If we expand our model so that \(x\) and \(y\) depend on t indirectly through two new functions \(u(t)\) and \(v(t)\) our previous statment is still correct. We need only expand the derivatives for \(x\) and \(y\). For example:</h3>
                    <h3>$$\frac{dx}{dt} = \frac{dx}{du}\frac{du}{dt} + \frac{dx}{dv}\frac{dv}{dt} $$</h3>
                </div>
            </section>

            <section class="slide-top">
                <div class="wrap">
                    <div class="grid sm">
                        <div class="column">
                            <h3>Example: Logistic Regression</h3>
                            <h3>
                                \begin{align}
                                z &= wx + b \\
                                y &= \sigma(z)\\
                                \mathcal{L} &= \frac{1}{2}(y-t)^2 \\
                                \mathcal{R} &= \frac{1}{2}w^2 \\
                                \mathcal{L}_{reg} &= \mathcal{L} + \lambda \mathcal{R}
                                \end{align}
                            </h3>
                        </div>
                        <div class="column bg-white shadow">
                            <h3>Computational Graph</h3>
                            <img class="alignright" src="media/reverseMode-1.png" alt="dual numbers 1">
                        </div>
                    </div>
                </div>
            </section>

            <section>
                <div class="grid">
                    <div class="column">
                        <h4>Now, let's calculate the derivatives going backwards through the graph. </h4>
                        <h4>
                            \begin{align}
                            \overline{\mathcal{L}}_{reg} &:= \frac{d\mathcal{L}_{reg}}{d\mathcal{L}_{reg}} = 1 \\
                            \overline{\mathcal{R}} &= \frac{d\mathcal{L}_{reg}}{d\mathcal{L}_{reg}}\frac{d\mathcal{L}_{reg}}{d \mathcal{R}} =\overline{\mathcal{L}}_{reg}\cdot \lambda \\
                            \overline{\mathcal{L}} &= \overline{\mathcal{L}}_{reg}\frac{d\mathcal{L}_{reg}}{d\mathcal{L}} = \overline{\mathcal{L}}_{reg}\cdot 1 \\
                            \overline{y} &= \overline{\mathcal{L}}\frac{d\mathcal{L}}{dy} = \overline{\mathcal{L}}(y-t) \\
                            \overline{z} &= \overline{y}\frac{dy}{dz} = \overline{y}\sigma'(z) \\
                            \overline{w} &= \overline{z}\frac{\partial z}{\partial w} + \overline{\mathcal{R}}\frac{d\mathcal{R}}{dw} = \overline{z}x + \overline{\mathcal{R}}w \\
                            \overline{b} &= \overline{z}\frac{\partial z}{\partial b} = \overline{z}
                            \end{align}
                        </h4>
                    </div>
                    <div class="column bg-white shadow">
                        <img class="alignright" src="media/reverseMode-1.png" alt="dual numbers 1">
                    </div>
                </div>
            </section>

            <section class="slide-top">
                <div class="alignleft">
                    <h1><strong>Summary</strong></h1>
                    <h3>First we pass our values (here \(x\)) through the model to get a prediction (here \(y\) with loss function values \(\mathcal{L}_{reg}\) ) </h3>
                    <br>
                    <h3>To optimize the model parameters w.r.t. the loss function, we now start at the end and calculate derivatives going from the output <i>backwards</i> towards the input).</h3>
                    <br>
                    <h3>This is called <u>Reverse Mode AD</u> or <u>backpropagation</u>. </h3>
                    <br>
                    <h3> For large models with many parameters and many nested functions, this methods is more efficient because we don't have to drage the derivatives through every step of the model on the forward pass when we apply the model to data </h3>
                    <br>
                    <h3>There are two ways to implement this approach: the <a href="https://pytorch.org/tutorials/beginner/examples_autograd/tf_two_layer_net.html">static graph</a> (used by TensorFlow) and <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">gradient tape</a> (used by Pytorch, Julia, everyone else).
                </div>
            </section>

            <section>
                <div class="grid">
                    <div class="column">
                        <h2>Example 2: Simple Neural Network</h2>
                        <h4>$$z_i = \sum_j w_{ij}^1x_j + b_i^1 $$</h4>
                        <h4>$$h_i = \sigma(z_i)$$</h4>
                        <h4>$$y_i = \sum_j w_{ij}^2h_j + b_i^2 $$</h4>
                        <h4>$$\mathcal{L} = \frac{1}{2}\sum_k(y_k-t_k)^2 $$</h4>
                    </div>
                    <div class="column bg-white shadow">
                        <img class="alignright" src="media/reverseMode-2.png" alt="reverse mode 2">
                    </div>
                </div>
            </section>

            <section>
                <div class="grid">
                    <div class="column">
                        <h3><strong>Index Form</strong></h3>
                        <h4>
                            \begin{align}
                            \overline{\mathcal{L}} &= 1\\
                            \overline{y_i} &= \overline{\mathcal{L}}(y_i-t_i) \\
                            \overline{w_{ij}^2} &= \overline{y_i}h_j \\
                            \overline{b_i^2} &= \overline{y_i} \\
                            \overline{h_i} &= \sum_k(\overline{y}_k w_{ki}^2)\\
                            \overline{z_i} &= \overline{h_i}\sigma'(z_i) \\
                            \overline{w_{ij}^1} &= \overline{z_i}x_j \\
                            \overline{b_i^1} &= \overline{z_i}
                            \end{align}
                        </h4>
                    </div>
                    <div class="column">
                        <h3><strong>Matrix Form</strong></h3>
                        <h4>
                            \begin{align}
                            \overline{\mathcal{L}} &= 1 \\
                            \overline{y} &= \overline{\mathcal{L}}(\vec{y}-\vec{t}\;) \\
                            \overline{W_2} &= \overline{y}\left(\vec{h}^T\right) \\
                            \overline{b_2} &= \overline{y} \\
                            \overline{h} &= W_2^T\overline{y}\\
                            \overline{z} &= \overline{y}\; .* \; \sigma'(\vec{z}) \\
                            \overline{W_1} &= \overline{z}\left(\vec{x}^T\right) \\
                            \overline{b_1} &= \overline{z}
                            \end{align}
                        </h4>
                        <h4>The rule is multiply by Matrix on forward pass and by Matrix-Transpose on backwards pass. </h4>
                    </div>
                    <div class="column bg-white shadow">
                        <h3><strong>Computational Graph</strong></h3>
                        <img class="alignright" src="media/reverseMode-2.png" alt="reverse mode 2">
                    </div>
                </div>
            </section>


            <section>
                <div class="alignleft">
                    <h3> This is great, but it requires that we create a new set of data structures in our code that can track the propagation of derivatives.</h3><br><br><br>
                    <h3> i.e. your <code>tf.Tensor</code> in TensorFlow and your <code>torch.Tensor</code> in PyTorch.</h3><br><br><br>
                    <h3> This is annoying. Why can't we just use numpy arrays?</h3><br><br><br>
                    <h3> Further, it means our machine learning library can't compose with other packages (e.g. SciPy) since we can't track derivatives through them.</h3>
                </div> 
            </section>


            <!-- Differentiable Programming -->
            <section class="aligncenter bg-apple">
                <h1><strong>Differentiable Programming and the Julia Language</strong></h1>
            </section>

            <section class="slide-top">
                <div class="aligncenter bg-white shadow">
                    <iframe src="https://julialang.org/" width="1200" height="1200"></iframe>
                    <!-- <img src="media/julialang.gif" width="1000">
                    -->
                </div>
            </section>

            <section class="slide-top">
                <div class="alignleft">
                    <h1><strong>Differentiable Programming (\(\partial P\))</strong></h1>
                    <h3>A key feature of Julia is its abstract type system </h3><br><br>
                    <h3>This let's you write generic functions that dispatch to whatever data type you have</h3><br><br>
                    <h3>Further, this means packages are composable out of the box.</h3><br><br>
                    <h3>The AD package for Julia (currently Zygote.jl) has been implemented <i>language wide</i> meaning you can differentiate through almost any generic Julia Code!</h3><br><br>
                    <h3><u>Examples:</u></h3>
                    <h4>&emsp;<a href="https://diffeq.sciml.ai/stable/">DifferentialEquations.jl</a> -- full fledged DiffeEq package</h4>
                    <h4>&emsp;<a href="https://measurementsjl.readthedocs.io/en/latest/">Measurements.jl</a> -- numerical types for tracking uncertainties</h4>
                    <h4>&emsp;<a href="https://painterqubits.github.io/Unitful.jl/stable/">Unitful.jl</a> -- numerical types for tracking units</h4>
                    <h4>&emsp;<a href="https://fluxml.ai/Flux.jl/stable/">Flux.jl</a> -- deep learning package</h4>
                    <h3> These are all excellent packages and compose with one another out of the box! </h3>
                </div>
            </section>

            <section class="slide-top">
                <div class="grid">
                    <div class="column">
                        <h1><u>Example:</u> Hand Written Sine Function</h1>
                        <h3>We implement a (bad) version of the sine function via its Taylor Series</h3>
                        <h3>$$ \sin(x) = x -\frac{x^3}{3!} + \frac{x^5}{5!} - \dots $$</h3><br><br>
                        <h3>This implementation has a <code>for</code> loop</h3><br><br>
                        <h3>it even has <code>if</code> statements!</h3><br><br>
                        <h3>Julia differentiaties right through!</h3><br><br>
                        <h3>Both forward-mode AD and reverse-mode AD give us machine precision</h3>
                    </div>
                    <div class="column bg-white shadow">
                        <img class="alignright" src="media/diffablep-1.png" alt="diffable p 1">
                        <img class="alignright" src="media/diffablep-2.png" alt="diffable p 2">
                    </div>
                </div>
            </section>

            <section class="aligncenter">
                <h1>Why does this matter?</h1>
            </section>

            <section class="aligncenter">
                <h1>If we can define the adjoint (like the transpose of a matrix) then we can compute gradients and optimize!</h1>
            </section>

            <section class="slide-top">
                <div class="wrap">
                    <div class="grid">
                        <div class="column">
                            <h2><u><strong>Application:</strong> Reverse-mode AD of ODE Solutions</u> </h2>
                            <h5>See <a href="https://arxiv.org/abs/1806.07366">this</a> excellent paper</h5><br>
                            <h5>Suppose we want to optimize some function \(L\) that depends on the result of an ODE solver, i.e. </h5>
                            <h5>
                                \begin{align}
                                L(z(t_1)) &:= L\left(z(t_0) + \int_{t_0}^{t_1}f(z(t),t, \theta) dt \right)\\
                                &= L\left(\text{ODESolve}(z(t_0), f, t_0, t_1, \theta) \right)
                                \end{align}
                            </h5>
                            <h5>We can derive an adjoint ODE via the method of Lagrange multipliers (see paper for details) solved in reverse time so that:</h5>
                            <h5>
                                \begin{align}
                                a(t) &= \frac{\partial L}{\partial z(t)} \\
                                \frac{dL}{d\theta} &= -\int_{t_1}^{t_0} a(t) \frac{\partial f}{\partial \theta} dt
                                \end{align}
                            </h5>
                            <h5>This allows us to perform forward pass to integrate the ODE and then pass the gradients backwards through the adjoint ODE! \(dL/d\theta\) can be obtained easily by automatic differentiation since \(f\) is given.</h5>
                            <h5>We can now use any standard optimization scheme to fit an ordinary differential equation to an arbitrary Loss function (such as \(\ell_2\)-norm between our integrated result and data. See <a href="https://julialang.org/blog/2019/01/fluxdiffeq/">this</a> awesome post for a detailed discussion of the Julia implementation.</h5>
                        </div>
                        <div class="column bg-white shadow">
                            <img class="aligncenter" src="media/odesad-1.png" alt="odes ad 1">
                        </div>
                    </div>
                </div>
            </section>


            <section class="aligncenter">
                <h1>Given we can now differentiate any code we write, how can we combine our scientific knowledge with machine learning methods to acheive better models?</h1>
            </section>


            <!-- Scientific Machine Learning -->
            <section class="bg-apple">
                <div class="aligncenter">
                    <h1><strong>Scientific Machine Learning</strong></h1>
                </div>
            </section>

            <section class="slide-top">
                <div class="alignleft">
                    <h1>It can be hard to tell what Machine Learning <i>actually</i> is. </h1>
                    <h3>Just like <i>quantum woo</i> in physics, there is a lot of <i>machine learning woo</i>. Compare:</h3>
                    <div class="wrap">
                        <div class="grid">
                            <div class="column bg-white shadow">
                                <img class="alignright" src="media/quantum-woo.png" alt="quantum woo">
                            </div>
                            <div class="column bg-white shadow">
                                <img class="alignright" src="media/machinelearning-woo.png" alt="machine learning woo">
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <section class="slide-top">
                <div class="wrap">
                    <div class="grid">
                        <div class="column">
                            <h3><u>Primer:</u> What is a Neural Network anyways?</h4>
                            <h4>A Neural Network is just a function made up of many composed functions </h4>
                            <h4> The simplest example is the <i>Multi-Layer Perceptron</i> which looks like this: </h4>
                            <h4> Each layer is of the form \( L_i(\vec{x}) = \sigma_i \left(\mathbf{W}_i \vec{x} + \vec{b}_i \right) \) </h4>
                            <h4> so that an MLP with \(n\) layers can be written as
                                $$NN(\vec{x}) = L_n\circ L_{n-1}\circ L_{n-2}\circ \dots \circ L_{2}\circ L_1(\vec{x}) $$
                            </h4>
                            <h4>\(\mathbf{W}_i\) are the weight matrices, \(\vec{b}_i\) are the bias vectors, and \(\sigma_i\) are the nonlinear activation functions (e.g. \(\tanh\) ). </h4>
                            <h4>Since we can now use AD, we can optimize the weights and biases to make our NN fit any function we want</h4>
                        </div>
                        <div class="column bg-white shadow">
                            <img class="alignright" src="media/NN-1.png" alt="NN 1">
                            <img class="alignright" src="media/NN-2.png" alt="NN 2">
                            <img class="alignright" src="media/NN-3.png" alt="NN 3">
                        </div>
                    </div>
                </div>
            </section>

            <section class="aligncenter">
                <div class="grid">
                    <div class="column">
                        <h2>In this form, our machine learning approach is a black box.</h2><br><br>
                        <h2>How can we incorporate our physics knowledge into these models to make them better?</h2><br><br>
                        <h2><u>In other words:</u> <i>Let's not throw away information someone already painstakingly collected experimentally/theoretically.</i></h2>
                    </div>
                    <div class="column">
                        <img class="aligncenter" src="media/xkcd-1.png" alt="xkcd 1">
                    </div>
                </div>
            </section>

            <section class="slide-top">
                <div class="wrap">
                    <div class="grid">
                        <div class="column">
                            <h3><strong>Method 1:</strong> <u>Physics Informed Neural Network</u> </h3>
                            <h4>Consider a driven spring system: \( x'' = -kx + 0.1\sin(x) \)</h4><br>
                            <h4>Imagine we don't know the r.h.s of above and we only have 4 badly sampled points.  </h4><br>
                            <h4>Can we predict the force and thereby predict the motion?</h4><br>
                            <h4>Naively, we may try to predict the force directly via optimizing a loss function like:</h4><br>
                            <h4> \( L(p) = \sum_i\left( NN(x(t_i), v(t_i), t_i)-F(x(t_i),v(t_i),t_i) \right)^2 \) </h4>
                        </div>
                        <div class="column">
                            <img class="alignright" src="media/physInformed-1.png" alt="phys informed 1">
                        </div>
                    </div>
                </div>
            </section>

            <section class="slide-top">
                <div class="wrap">
                    <div class="grid">
                        <div class="column">
                            <h4>Our poorly sampled data leads to a fit with the the right general shape, but doesn't capture the extreme behavior. </h4><br><br>
                            <h4>As physicists we <i>know</i> that Hooke's Law, \(F_{\text{spring}} = -kx \) captures the behavior of a spring-mass system. </h4><br><br>
                            <h4>Further, the spring constant \(k\) is a commonly measured parameter for many systems. </h4><br><br>
                            <h4>Right now we do not include any of this information in our NN model. </h4><br><br>
                            <h4>What if someone told us that \(k=0.1\)? Could we make a better model?</h4><br><br>
                        </div>
                        <div class="column">
                            <img class="alignright" src="media/physInformed-2.png" alt="phys informed 2">
                            <img class="alignright" src="media/physInformed-3.png" alt="phys informed 3">
                        </div>
                    </div>
                </div>
            </section>

            <section class="slide-top">
                <div class="wrap">
                    <div class="grid">
                        <div class="column">
                            <h4><strong>Solution:</strong><u>Add physics regularization term to loss function</u></h4>
                            <img class="alignright" src="media/physInformed-4.png" alt="phys informed 4">
                            <img class="alignright" src="media/physInformed-5.png" alt="phys informed 5">
                            <h4>Now we can control the strength of our physics model by tweaking \(\lambda\)</h4>
                        </div>
                        <div class="column">
                            <img class="alignright" src="media/physInformed-6.png" alt="phys informed 6">
                        </div>
                    </div>
                </div>
            </section>


            <section class="slide-top">
                <div class="wrap">
                    <div class="grid">
                        <div class="column">
                            <h3><strong>Method 2:</strong> <u>Neural ODEs</u> </h3>
                            <h4>We know we can track gradients through ODE solvers</h4><br>
                            <h4>Can we fit an ODE to time-series data using a NN?</h4><br>
                            <h4>i.e. $$ y'(x) = NN(x) $$ </h4><br>

                            <img class="alignright" src="media/neuralODE-1.png" alt="neural ode 1">
                            <img class="alignright" src="media/neuralODE-2.png" alt="neural ode 2">
                        </div>
                        <div class="column">
                            <h4>Pre-training result </h4>
                            <img class="alignright" src="media/neuralODE-3.png" alt="neural ode 3">
                        </div>
                    </div>
                </div>
            </section>

            <section class="aligncenter">
                <h1><strong>Learning the ODE system</strong></h1>
                <div class="grid bg-white shadow">
                    <img class="alignright" src="media/neuralODE-4.png" alt="neural ode 4">
                </div>
            </section>


            <section class="slide-top">
                <div class="wrap">
                    <div class="grid">
                        <div class="column">
                            <h3><strong>Method 3:</strong> <u>Universal Differential Equations</u> </h3>
                            <h4>Checkout <a href="https://arxiv.org/abs/2001.04385">this</a> awesome paper</h4><br>
                            <h4>Consider the Lotka-Volterra predator-prey model
                                \begin{align}
                                \dot{x} &= \alpha x - \beta xy \\
                                \dot{y} &= \gamma xy - \delta y
                                \end{align}
                            </h4><br>
                            <h4>Suppose a clever scientist has already (by some other means) evaluated the prey birth rate \(\alpha\) and the predator death rate \(\delta\)</h4>
                            <h4>Can we fit NN to learn only the missing terms?
                                \begin{align}
                                \dot{x} &= \alpha x + NN_1(x,y;p_1) \\
                                \dot{y} &= -\delta y + NN_2(x, y; p_2)
                                \end{align}
                            </h4>
                        </div>
                        <div class="column">
                            <h4>Training Data </h4>
                            <img class="alignright" src="media/ude-1.png" alt="ude 1">
                        </div>
                    </div>
                </div>
            </section>

             <section class="slide-top">
                <div class="wrap">
                    <div class="grid">
                        <div class="column">
                            <img class="alignright" src="media/ude-2.png" alt="ude 2"><br><br>
                            <img class="alignright" src="media/ude-4.png" alt="ude 4">
                        </div>
                        <div class="column">
                            <h3>Pre-training Fit</h3>
                            <img class="alignright" src="media/ude-3.png" alt="ude 3">
                        </div>
                    </div>
                </div>
            </section>

            <section class="aligncenter">
                <h1><strong>Post-training Fit</strong></h1>
                <div class="grid bg-white shadow">
                    <img class="alignright" src="media/ude-5.png" alt="ude 5">
                </div>
            </section>

            <section class="aligncenter">
                <h1><strong>It gets better</strong></h1>
                <h2>Now that we have <i>learned</i> the missing terms, we extract their functional form</h2>
            </section>

            <section class="slide-top">
                <div class="wrap">
                    <div class="grid">
                        <div class="column">
                            <h3><strong>SINDy</strong>: <u>S</u>parse <u>I</u>dentification of <u>N</u>onlinear <u>Dy</u>namics </h3>
                            <h4>Create a basis set of combinations of our variables (here \(x\) and \(y\))</h4><br><br>
                            <h4>Perform a <i>sparse</i> regression (i.e. that minimizes weights) by taking linear combination of basis terms.</h4><br><br>
                            <h4><strong>Result:</strong> We correctly infer that the missing terms are ofthe form \(xy\) in the first equations and \(xy\) in the second equation.</h4>
                        </div>
                        <div class="column">
                            <img class="alignright" src="media/sindy-1.png" alt="sindy 1">
                        </div>
                    </div>
                </div>
            </section>

            <section class="aligncenter">
                <h1>Now that we have a functional form, we can do a new fit to get the coefficients right.</h1><br><br>
                <h2>The result is a <i>deterministic</i> model that has insane generalizability!</h2>
            </section>


            <section class="aligncenter">
                <h1><strong>Post-SINDy Extrapolation</strong></h1>
                <div class="grid bg-white shadow">
                    <img class="alignright" src="media/ude-6.png" alt="ude 6">
                </div>
                <h3> Have I just put all of us out of business?</h3><br><br>
                <h3> Probably not, but this is still pretty cool.</h3>
            </section>


            <section class="slide-top">
                <div class="wrap">
                    <div class="grid">
                        <div class="column">
                            <h3><strong>Method 4:</strong> <u>Hamiltonian Neural Networks</u> </h3>
                            <h4>Checkout <a href="https://arxiv.org/abs/1906.01563">this</a> awesome paper</h4><br>
                            <h4>Recall Hamilton's Equations:
                                \begin{align}
                                \dot{q} &= \frac{dH}{dp} & \dot{p} &= -\frac{\partial H}{\partial q}
                                \end{align}
                            </h4>
                            <h4>Suppose we have sampled some time series so that we have some data for \(q\) and \(p\).</h4><br>
                            <h4>If we want to predcit the Hamiltonian function \(H\), we could directly train a Neural Network.</h4><br>
                            <h4>A different (better) approach is to utilize our AD system to train the output of the NN to obey Hamilton's equations, i.e.</h4><br>
                            <h4>\( L_{\text{HNN}} = \left\Vert \frac{\partial H}{\partial p} - \frac{\partial q}{\partial t} \right\Vert_2 + \left\Vert \frac{\partial H}{\partial q}+ \frac{\partial p}{\partial t} \right\Vert_2 \) </h4>
                        </div>
                        <div class="column">
                            <h4><u>Three Sample Problems:</u> Ideal Spring, Ideal Pendulum, Real Pendulum </h4>
                            <img class="alignright" src="media/hnn-1.png" alt="hnn 1">
                            <h4>Because we have learned a representation for \(H\) we solve the same system for any initial energy by adding a constant shift to \(H\) and proceeding!</h4>
                        </div>
                    </div>
                </div>
            </section>

            <section >
                <div class="wrap">
                    <div class="grid">
                        <div class="column">
                            <h3>If you don't have \(q\) and \(p\) directly, that's okay.</h3>
                            <h3>Have your NN learn internal representations of \(q\) and \(p\) and then output \(H\) from those.</h3>
                            <h3><strong>Example:</strong> <u>learning physics from video</u></h3>
                            <img class="alignright" src="media/hnn-2.png" alt="hnn 2">
                        </div>
                        <div class="column">
                            <img class="alignright" src="media/hnn-3.png" alt="hnn 3">
                        </div>
                    </div>
                </div>
            </section>


            <section class="aligncenter">
                <h1>This is super easy in Julia</h1>
                <div class="grid bg-white shadow">
                    <div class="column">
                        <img class="alignright" src="media/hnn-4.png" alt="hnn 4">
                    </div>
                    <div class="column">
                        <h3>Source code</h3>
                        <img class="alignright" src="media/hnn-5.png" alt="hnn 5">
                    </div>
                </div>
            </section>


            <section class="aligncenter">
                <div class="wrap">
                    <h1> Thanks for listening!</h1>
                    <img class="aligncenter" src="media/xkcd-2.png" alt="xkcd 2">
                </div>
            </section>
    </main>
    <!-- Required -->
    <script src="../static/js/webslides.js"></script>
    <script>
      window.ws = new WebSlides();
    </script>
  </body>
</html>
